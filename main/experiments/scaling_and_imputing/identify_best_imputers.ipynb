{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import DATASET_PATH\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from textwrap import wrap\n",
    "\n",
    "from main.constants import PROLACTIN, VITAMINE_D, IGF, PCO, CATEGORICAL_ATTRIBUTES, CONTINUOUS_ATTRIBUTES\n",
    "\n",
    "pd.set_option('display.max_columns', None) # enable showing all columns of the df\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.expand_frame_repr\", True)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCO 0-healthy control, 1-PCOS, 2-FHA 3-POF, 4-High Andro</th>\n",
       "      <th>IGF-1 ng/ml (N: 100-311)</th>\n",
       "      <th>proBNP</th>\n",
       "      <th>AMH (ng/ml) *7,14=pmol/l</th>\n",
       "      <th>weight</th>\n",
       "      <th>height (cm)</th>\n",
       "      <th>BMI</th>\n",
       "      <th>systolic BP (ciśnienie skurczowe)</th>\n",
       "      <th>diastolic BP (ciśnienie rozskurczowe)</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Waist Circumference (WC)</th>\n",
       "      <th>WC&gt;88</th>\n",
       "      <th>Hip Circumference (HC)</th>\n",
       "      <th>WHR (Waist/Hip ratio)</th>\n",
       "      <th>WHR&gt;0,85 (WHO)</th>\n",
       "      <th>WHR&gt;0,8 (NIDDK)</th>\n",
       "      <th>WHTR (Waist/Height Ratio)</th>\n",
       "      <th>WHTR&gt;0,5</th>\n",
       "      <th>overweight/obesity 0-normal/low, 1-overweight, 2-obesity</th>\n",
       "      <th>irregular cycles (0-nie, 1-tak)</th>\n",
       "      <th>ovulation (0-brak, 1-obecna)</th>\n",
       "      <th>PCO ovary morfology in USG (0-brak, 1--obecna)</th>\n",
       "      <th>stromal hypertrophy in ovary (0-brak, 1-obecny)</th>\n",
       "      <th>acne</th>\n",
       "      <th>hirsutism</th>\n",
       "      <th>FG score (Ferriman-Gallway score - stopień androgenizacji)</th>\n",
       "      <th>hyperandrogenism</th>\n",
       "      <th>elevated DHT</th>\n",
       "      <th>hypothyroidism</th>\n",
       "      <th>Volume of the thyroid  Right Lobe</th>\n",
       "      <th>Volume of the thyroid  Left Lobe</th>\n",
       "      <th>thyroid volume</th>\n",
       "      <th>nodules 0-lack, 1-RL,  2-LL, 3-both</th>\n",
       "      <th>chronic thyroiditis</th>\n",
       "      <th>Vole of the Right Ovary</th>\n",
       "      <th>Volume of the  Left Ovary</th>\n",
       "      <th>ovaries volume - total</th>\n",
       "      <th>follicules &gt;12</th>\n",
       "      <th>WBC x10^3/ul</th>\n",
       "      <th>neutrophil x10^3/ul</th>\n",
       "      <th>lymphocytes x10^3/ul</th>\n",
       "      <th>monocytes x10^3/ul</th>\n",
       "      <th>eosinocytes x10^3/ul</th>\n",
       "      <th>basophils x10^3/ul</th>\n",
       "      <th>% neutrophil</th>\n",
       "      <th>% lymphocytes</th>\n",
       "      <th>% monocytes</th>\n",
       "      <th>%eosinocytes</th>\n",
       "      <th>%basophils</th>\n",
       "      <th>RBC x10^6ul</th>\n",
       "      <th>Hemoglobin [g/dl]</th>\n",
       "      <th>hematocrit [%]</th>\n",
       "      <th>HTC/Hb</th>\n",
       "      <th>MCV fl</th>\n",
       "      <th>MCH pg</th>\n",
       "      <th>MCHC g/dl</th>\n",
       "      <th>RDW-CV %</th>\n",
       "      <th>PLT x10^3/ul</th>\n",
       "      <th>PDW fl</th>\n",
       "      <th>MPV fl</th>\n",
       "      <th>P-LCR %</th>\n",
       "      <th>PLT/WBC</th>\n",
       "      <th>MPV/PLT</th>\n",
       "      <th>PLR</th>\n",
       "      <th>limf/mono</th>\n",
       "      <th>NLR (stosunek neutrofili do limfocytów)</th>\n",
       "      <th>L/WCC (leukocyty do całkowitej liczby krwinek białych)</th>\n",
       "      <th>eos/leukocyty</th>\n",
       "      <th>sodium mmol/l</th>\n",
       "      <th>potassium mmol/l</th>\n",
       "      <th>calcium mg/dl</th>\n",
       "      <th>phosphorus mg/dl</th>\n",
       "      <th>creatinine mg/dl</th>\n",
       "      <th>CRP mg/l</th>\n",
       "      <th>ALT U/l</th>\n",
       "      <th>AST U/l</th>\n",
       "      <th>Bilirubin mg/dl</th>\n",
       "      <th>CHOL mmol/l</th>\n",
       "      <th>CHOL&gt;200</th>\n",
       "      <th>HDL mmol/l</th>\n",
       "      <th>HDL&lt;50</th>\n",
       "      <th>LDL mmol/l</th>\n",
       "      <th>LDL&gt;135</th>\n",
       "      <th>TG mmol/l</th>\n",
       "      <th>Atherogenic index (AI) (LDL-C/HDL-C)</th>\n",
       "      <th>coronary risk index (CRI) (TG/HDL-C)</th>\n",
       "      <th>VAI - Visceral adiposity index</th>\n",
       "      <th>BAI - Body adiposity index</th>\n",
       "      <th>LAP INDEX - Lipid accumulation product index</th>\n",
       "      <th>TyG Index - Trigliceride-glucose index</th>\n",
       "      <th>AIP -Atherogenic index of plasma</th>\n",
       "      <th>UIBC ug/dl</th>\n",
       "      <th>ferrum ug/dl</th>\n",
       "      <th>TIBC</th>\n",
       "      <th>TSAT</th>\n",
       "      <th>ferritin ng/ml</th>\n",
       "      <th>glucose 0 mg/dl</th>\n",
       "      <th>glucose  120 mg/dl</th>\n",
       "      <th>insulin 0 uU/ml</th>\n",
       "      <th>Insulin 120 uU/ml</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Matsuda</th>\n",
       "      <th>QUICKI (N&lt;0,357)</th>\n",
       "      <th>TSH mIU/L</th>\n",
       "      <th>FT3 pmol/l</th>\n",
       "      <th>FT4 pmol/l</th>\n",
       "      <th>Anty-TPO IU/ml</th>\n",
       "      <th>Anty-TG IU/ml</th>\n",
       "      <th>FSH mlU/ml</th>\n",
       "      <th>LH</th>\n",
       "      <th>LH/FSH</th>\n",
       "      <th>prolactin</th>\n",
       "      <th>DHEA-S ug/dl</th>\n",
       "      <th>testosterone nmol/l</th>\n",
       "      <th>T (ng/ml)</th>\n",
       "      <th>T/SHBG</th>\n",
       "      <th>E(pg/ml)/T(ng/ml)/</th>\n",
       "      <th>Parathormone pg/ml</th>\n",
       "      <th>cortisol nmol/l  8:00</th>\n",
       "      <th>cortisol nmol/l 18:00</th>\n",
       "      <th>Estradiol pg/ml</th>\n",
       "      <th>SHBG nmol/l</th>\n",
       "      <th>SHBG&gt;110</th>\n",
       "      <th>FTI (free testosterone index)</th>\n",
       "      <th>ACTH pg/ml</th>\n",
       "      <th>HbA1c %</th>\n",
       "      <th>vitamin 25-OH D ng/ml</th>\n",
       "      <th>Androstendione ng/ml</th>\n",
       "      <th>17-OH-progesterone ng/ml</th>\n",
       "      <th>Dihydrotestosterone pg/ml (N&lt;368)</th>\n",
       "      <th>Testosterone/DHT</th>\n",
       "      <th>T/A (testosterone/androstendione)</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1</td>\n",
       "      <td>167.0</td>\n",
       "      <td>40.12</td>\n",
       "      <td>8.140000</td>\n",
       "      <td>67.5</td>\n",
       "      <td>173.0</td>\n",
       "      <td>22.553376</td>\n",
       "      <td>109.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.442197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5600</td>\n",
       "      <td>4.920</td>\n",
       "      <td>9.4800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.7000</td>\n",
       "      <td>8.75</td>\n",
       "      <td>27.4500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.03</td>\n",
       "      <td>54.8</td>\n",
       "      <td>30.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.66</td>\n",
       "      <td>12.8</td>\n",
       "      <td>38.3</td>\n",
       "      <td>2.992188</td>\n",
       "      <td>82.2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>33.4</td>\n",
       "      <td>13.8</td>\n",
       "      <td>236.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>11.1</td>\n",
       "      <td>34.3</td>\n",
       "      <td>47.676768</td>\n",
       "      <td>0.047034</td>\n",
       "      <td>156.291391</td>\n",
       "      <td>2.649123</td>\n",
       "      <td>1.794702</td>\n",
       "      <td>0.305051</td>\n",
       "      <td>0.026263</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4.91</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>5.37888</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.40498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.503248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.07255</td>\n",
       "      <td>1.040860</td>\n",
       "      <td>1.021505</td>\n",
       "      <td>0.808293</td>\n",
       "      <td>31.132948</td>\n",
       "      <td>19.842175</td>\n",
       "      <td>4.515607</td>\n",
       "      <td>0.009241</td>\n",
       "      <td>342.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>5.45</td>\n",
       "      <td>54.88</td>\n",
       "      <td>1.184198</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.373012</td>\n",
       "      <td>0.98</td>\n",
       "      <td>5.320</td>\n",
       "      <td>13.86</td>\n",
       "      <td>12.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>23.8</td>\n",
       "      <td>4.407407</td>\n",
       "      <td>315.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.071588</td>\n",
       "      <td>46.628125</td>\n",
       "      <td>41.97</td>\n",
       "      <td>286.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>44.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.19</td>\n",
       "      <td>20.02</td>\n",
       "      <td>5.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.91</td>\n",
       "      <td>3.71</td>\n",
       "      <td>532.0</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.541455</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>1</td>\n",
       "      <td>345.0</td>\n",
       "      <td>17.60</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>75.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>30.043262</td>\n",
       "      <td>136.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.887755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.550633</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5240</td>\n",
       "      <td>4.080</td>\n",
       "      <td>11.6040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0832</td>\n",
       "      <td>6.47</td>\n",
       "      <td>16.5532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.18</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>50.6</td>\n",
       "      <td>36.7</td>\n",
       "      <td>10.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.66</td>\n",
       "      <td>13.3</td>\n",
       "      <td>41.7</td>\n",
       "      <td>3.135338</td>\n",
       "      <td>89.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>31.9</td>\n",
       "      <td>12.9</td>\n",
       "      <td>256.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>25.8</td>\n",
       "      <td>49.420849</td>\n",
       "      <td>0.039844</td>\n",
       "      <td>134.736842</td>\n",
       "      <td>3.584906</td>\n",
       "      <td>1.378947</td>\n",
       "      <td>0.366795</td>\n",
       "      <td>0.021236</td>\n",
       "      <td>143.0</td>\n",
       "      <td>3.78</td>\n",
       "      <td>9.60</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.74</td>\n",
       "      <td>3.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.64626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.55160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.781754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65482</td>\n",
       "      <td>1.148333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.737991</td>\n",
       "      <td>44.025316</td>\n",
       "      <td>18.989780</td>\n",
       "      <td>4.352417</td>\n",
       "      <td>-0.014723</td>\n",
       "      <td>289.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>14.15</td>\n",
       "      <td>67.56</td>\n",
       "      <td>3.633580</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.315678</td>\n",
       "      <td>2.85</td>\n",
       "      <td>6.070</td>\n",
       "      <td>18.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2.153846</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.403458</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>76.835714</td>\n",
       "      <td>29.05</td>\n",
       "      <td>594.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.63</td>\n",
       "      <td>113.10</td>\n",
       "      <td>5.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.02</td>\n",
       "      <td>3.81</td>\n",
       "      <td>452.0</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.278884</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>2</td>\n",
       "      <td>298.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>21.303949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4125</td>\n",
       "      <td>6.336</td>\n",
       "      <td>12.7485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.5500</td>\n",
       "      <td>5.83</td>\n",
       "      <td>14.3800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.54</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.05</td>\n",
       "      <td>58.4</td>\n",
       "      <td>15.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.97</td>\n",
       "      <td>12.1</td>\n",
       "      <td>34.9</td>\n",
       "      <td>2.884298</td>\n",
       "      <td>87.9</td>\n",
       "      <td>30.5</td>\n",
       "      <td>34.7</td>\n",
       "      <td>12.5</td>\n",
       "      <td>195.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>10.7</td>\n",
       "      <td>31.4</td>\n",
       "      <td>25.862069</td>\n",
       "      <td>0.054872</td>\n",
       "      <td>163.865546</td>\n",
       "      <td>0.832168</td>\n",
       "      <td>3.705882</td>\n",
       "      <td>0.157825</td>\n",
       "      <td>0.061008</td>\n",
       "      <td>142.0</td>\n",
       "      <td>4.32</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5.7</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>3.23250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.70676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.106808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88062</td>\n",
       "      <td>0.648485</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.881983</td>\n",
       "      <td>29.878788</td>\n",
       "      <td>10.567440</td>\n",
       "      <td>4.405528</td>\n",
       "      <td>0.072551</td>\n",
       "      <td>306.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>5.94</td>\n",
       "      <td>44.82</td>\n",
       "      <td>1.261333</td>\n",
       "      <td>9.52</td>\n",
       "      <td>0.369237</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.089</td>\n",
       "      <td>14.09</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>155.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288184</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>492.739997</td>\n",
       "      <td>32.92</td>\n",
       "      <td>323.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>71.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>12.83</td>\n",
       "      <td>5.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.67</td>\n",
       "      <td>522.0</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.621118</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1</td>\n",
       "      <td>208.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>20.195578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.363095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.53</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>51.3</td>\n",
       "      <td>35.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.85</td>\n",
       "      <td>11.9</td>\n",
       "      <td>34.3</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>89.1</td>\n",
       "      <td>30.9</td>\n",
       "      <td>34.7</td>\n",
       "      <td>11.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>12.1</td>\n",
       "      <td>41.8</td>\n",
       "      <td>5.382436</td>\n",
       "      <td>0.636842</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>3.676471</td>\n",
       "      <td>1.448000</td>\n",
       "      <td>0.354108</td>\n",
       "      <td>0.025496</td>\n",
       "      <td>141.0</td>\n",
       "      <td>4.03</td>\n",
       "      <td>9.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.90486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.68090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.172240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48547</td>\n",
       "      <td>1.292308</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.442283</td>\n",
       "      <td>24.857143</td>\n",
       "      <td>1.456410</td>\n",
       "      <td>4.107774</td>\n",
       "      <td>-0.179445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.944938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.387168</td>\n",
       "      <td>1.17</td>\n",
       "      <td>5.510</td>\n",
       "      <td>22.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1.158537</td>\n",
       "      <td>358.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.374640</td>\n",
       "      <td>0.018414</td>\n",
       "      <td>226.884614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>70.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.84</td>\n",
       "      <td>7.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.64</td>\n",
       "      <td>299.0</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1</td>\n",
       "      <td>454.0</td>\n",
       "      <td>32.18</td>\n",
       "      <td>7.336134</td>\n",
       "      <td>88.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>32.323232</td>\n",
       "      <td>144.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3840</td>\n",
       "      <td>3.870</td>\n",
       "      <td>10.2540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.2800</td>\n",
       "      <td>17.11</td>\n",
       "      <td>34.3900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.64</td>\n",
       "      <td>4.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>71.3</td>\n",
       "      <td>22.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.93</td>\n",
       "      <td>12.4</td>\n",
       "      <td>36.4</td>\n",
       "      <td>2.935484</td>\n",
       "      <td>92.6</td>\n",
       "      <td>31.6</td>\n",
       "      <td>34.1</td>\n",
       "      <td>12.3</td>\n",
       "      <td>302.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>25.8</td>\n",
       "      <td>45.481928</td>\n",
       "      <td>0.033775</td>\n",
       "      <td>198.684211</td>\n",
       "      <td>5.066667</td>\n",
       "      <td>3.118421</td>\n",
       "      <td>0.228916</td>\n",
       "      <td>0.007530</td>\n",
       "      <td>141.0</td>\n",
       "      <td>4.07</td>\n",
       "      <td>9.31</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>4.73238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.52574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.854944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76772</td>\n",
       "      <td>1.871186</td>\n",
       "      <td>1.152542</td>\n",
       "      <td>0.870078</td>\n",
       "      <td>42.606061</td>\n",
       "      <td>24.567040</td>\n",
       "      <td>4.381401</td>\n",
       "      <td>0.061657</td>\n",
       "      <td>306.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>12.61</td>\n",
       "      <td>22.28</td>\n",
       "      <td>2.926765</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.325326</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.140</td>\n",
       "      <td>16.21</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.072727</td>\n",
       "      <td>504.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.634006</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>29.968182</td>\n",
       "      <td>45.62</td>\n",
       "      <td>341.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.17</td>\n",
       "      <td>24.53</td>\n",
       "      <td>4.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>3.51</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PCO 0-healthy control, 1-PCOS, 2-FHA 3-POF, 4-High Andro  IGF-1 ng/ml (N: 100-311)  proBNP  AMH (ng/ml) *7,14=pmol/l  weight  height (cm)        BMI  systolic BP (ciśnienie skurczowe)  diastolic BP (ciśnienie rozskurczowe)  Hypertension  Waist Circumference (WC)  WC>88  Hip Circumference (HC)  WHR (Waist/Hip ratio)  WHR>0,85 (WHO)  WHR>0,8 (NIDDK)  WHTR (Waist/Height Ratio)  WHTR>0,5  overweight/obesity 0-normal/low, 1-overweight, 2-obesity  irregular cycles (0-nie, 1-tak)  ovulation (0-brak, 1-obecna)  PCO ovary morfology in USG (0-brak, 1--obecna)  stromal hypertrophy in ovary (0-brak, 1-obecny)  acne  hirsutism  FG score (Ferriman-Gallway score - stopień androgenizacji)  hyperandrogenism  elevated DHT  hypothyroidism  Volume of the thyroid  Right Lobe  Volume of the thyroid  Left Lobe  thyroid volume  nodules 0-lack, 1-RL,  2-LL, 3-both  chronic thyroiditis  Vole of the Right Ovary  Volume of the  Left Ovary  ovaries volume - total  follicules >12  WBC x10^3/ul  \\\n",
       "554                                                  1                            167.0   40.12                  8.140000    67.5        173.0  22.553376                              109.0                                   74.0           0.0                      76.5    0.0                    85.0               0.900000             1.0              1.0                   0.442197       0.0                                                0.0                                     1.0                           0.0                                             1.0                                              0.0   1.0        1.0                                               18.0                        1.0           1.0             1.0                             4.5600                             4.920          9.4800                                  0.0                  1.0                  18.7000                       8.75                 27.4500             1.0          4.95   \n",
       "491                                                  1                            345.0   17.60                  2.523810    75.0        158.0  30.043262                              136.0                                   79.0           0.0                      87.0    0.0                    98.0               0.887755             1.0              1.0                   0.550633       1.0                                                2.0                                     1.0                           0.0                                             0.0                                              0.0   1.0        1.0                                               12.0                        1.0           1.0             1.0                             7.5240                             4.080         11.6040                                  1.0                  1.0                  10.0832                       6.47                 16.5532             0.0          5.18   \n",
       "848                                                  2                            298.0     NaN                       NaN    58.0        165.0  21.303949                                NaN                                    NaN           NaN                      70.0    0.0                    79.0               0.886076             1.0              1.0                   0.424242       0.0                                                0.0                                     1.0                           0.0                                             0.0                                              0.0   0.0        0.0                                                0.0                        0.0           0.0             0.0                             6.4125                             6.336         12.7485                                  0.0                  1.0                   8.5500                       5.83                 14.3800             0.0          7.54   \n",
       "333                                                  1                            208.0     NaN                       NaN    57.0        168.0  20.195578                                NaN                                    NaN           NaN                      61.0    0.0                    72.0               0.847222             0.0              1.0                   0.363095       0.0                                                0.0                                     NaN                           NaN                                             NaN                                              NaN   NaN        NaN                                                NaN                        NaN           NaN             NaN                                NaN                               NaN             NaN                                  NaN                  NaN                      NaN                        NaN                     NaN             NaN          3.53   \n",
       "559                                                  1                            454.0   32.18                  7.336134    88.0        165.0  32.323232                              144.0                                   66.0           1.0                      90.0    1.0                   100.0               0.900000             1.0              1.0                   0.545455       1.0                                                2.0                                     1.0                           0.0                                             1.0                                              1.0   1.0        0.0                                                2.0                        1.0           1.0             0.0                             6.3840                             3.870         10.2540                                  0.0                  1.0                  17.2800                      17.11                 34.3900             1.0          6.64   \n",
       "\n",
       "     neutrophil x10^3/ul  lymphocytes x10^3/ul  monocytes x10^3/ul  eosinocytes x10^3/ul  basophils x10^3/ul  % neutrophil   % lymphocytes   % monocytes  %eosinocytes   %basophils   RBC x10^6ul  Hemoglobin [g/dl]  hematocrit [%]    HTC/Hb  MCV fl  MCH pg  MCHC g/dl  RDW-CV %  PLT x10^3/ul  PDW fl  MPV fl  P-LCR %    PLT/WBC   MPV/PLT         PLR  limf/mono  NLR (stosunek neutrofili do limfocytów)  L/WCC (leukocyty do całkowitej liczby krwinek białych)  eos/leukocyty  sodium mmol/l  potassium mmol/l  calcium mg/dl  phosphorus mg/dl  creatinine mg/dl  CRP mg/l  ALT U/l  AST U/l  Bilirubin mg/dl  CHOL mmol/l  CHOL>200  HDL mmol/l  HDL<50  LDL mmol/l  LDL>135  TG mmol/l  Atherogenic index (AI) (LDL-C/HDL-C)   coronary risk index (CRI) (TG/HDL-C)  VAI - Visceral adiposity index  BAI - Body adiposity index  LAP INDEX - Lipid accumulation product index  TyG Index - Trigliceride-glucose index  AIP -Atherogenic index of plasma  UIBC ug/dl  ferrum ug/dl   TIBC  TSAT  ferritin ng/ml  \\\n",
       "554                 2.71                  1.51                0.57                  0.13                0.03           54.8            30.5         11.5            2.6          0.6         4.66               12.8            38.3  2.992188    82.2    27.5       33.4      13.8         236.0    13.5    11.1     34.3  47.676768  0.047034  156.291391   2.649123                                 1.794702                                           0.305051            0.026263          144.0              4.91           9.77              3.06              0.89       0.3     20.0     26.0             0.51      5.37888       1.0     2.40498     0.0    2.503248      0.0    1.07255                               1.040860                              1.021505                        0.808293                   31.132948                                     19.842175                                4.515607                          0.009241       342.0          52.0    NaN   NaN             7.0   \n",
       "491                 2.62                  1.90                0.53                  0.11                0.02           50.6            36.7         10.2            2.1          0.4         4.66               13.3            41.7  3.135338    89.5    28.5       31.9      12.9         256.0    11.3    10.2     25.8  49.420849  0.039844  134.736842   3.584906                                 1.378947                                           0.366795            0.021236          143.0              3.78           9.60              3.78              0.74       3.6     23.0     20.0             0.25      3.64626       0.0     1.55160     0.0    1.781754      0.0    0.65482                               1.148333                              0.966667                        0.737991                   44.025316                                     18.989780                                4.352417                         -0.014723       289.0          69.0  357.0  19.0            56.0   \n",
       "848                 4.41                  1.19                1.43                  0.46                0.05           58.4            15.8         19.0            6.1          0.7         3.97               12.1            34.9  2.884298    87.9    30.5       34.7      12.5         195.0    13.1    10.7     31.4  25.862069  0.054872  163.865546   0.832168                                 3.705882                                           0.157825            0.061008          142.0              4.32           9.39              3.92              0.77       5.7     27.0     22.0             0.66      3.23250       0.0     1.70676     0.0    1.106808      0.0    0.88062                               0.648485                              1.181818                        0.881983                   29.878788                                     10.567440                                4.405528                          0.072551       306.0          30.0  336.0   9.0            71.0   \n",
       "333                 1.81                  1.25                0.34                  0.09                0.02           51.3            35.4          9.6            2.5          0.6         3.85               11.9            34.3  2.882353    89.1    30.9       34.7      11.9          19.0    16.5    12.1     41.8   5.382436  0.636842   15.200000   3.676471                                 1.448000                                           0.354108            0.025496          141.0              4.03           9.55               NaN              0.76       NaN     12.0     15.0              NaN      3.90486       0.0     1.68090     0.0    2.172240      0.0    0.48547                               1.292308                              0.661538                        0.442283                   24.857143                                      1.456410                                4.107774                         -0.179445         NaN           NaN    NaN   NaN            27.0   \n",
       "559                 4.74                  1.52                0.30                  0.05                0.03           71.3            22.9          4.5            0.8          0.5         3.93               12.4            36.4  2.935484    92.6    31.6       34.1      12.3         302.0    11.2    10.2     25.8  45.481928  0.033775  198.684211   5.066667                                 3.118421                                           0.228916            0.007530          141.0              4.07           9.31              2.71              0.75       1.2     16.0     17.0             0.26      4.73238       0.0     1.52574     0.0    2.854944      0.0    0.76772                               1.871186                              1.152542                        0.870078                   42.606061                                     24.567040                                4.381401                          0.061657       306.0          72.0  379.0  19.0            29.0   \n",
       "\n",
       "     glucose 0 mg/dl  glucose  120 mg/dl  insulin 0 uU/ml  Insulin 120 uU/ml      HOMA  Matsuda  QUICKI (N<0,357)  TSH mIU/L  FT3 pmol/l  FT4 pmol/l  Anty-TPO IU/ml  Anty-TG IU/ml  FSH mlU/ml    LH    LH/FSH  prolactin  DHEA-S ug/dl  testosterone nmol/l  T (ng/ml)    T/SHBG  E(pg/ml)/T(ng/ml)/  Parathormone pg/ml  cortisol nmol/l  8:00  cortisol nmol/l 18:00  Estradiol pg/ml  SHBG nmol/l  SHBG>110  FTI (free testosterone index)  ACTH pg/ml  HbA1c %  vitamin 25-OH D ng/ml  Androstendione ng/ml  17-OH-progesterone ng/ml  Dihydrotestosterone pg/ml (N<368)  Testosterone/DHT  T/A (testosterone/androstendione)   age  \n",
       "554             88.0               139.0             5.45              54.88  1.184198     7.80          0.373012       0.98       5.320       13.86            12.0          278.0         5.4  23.8  4.407407      315.0         466.0                  3.2   0.922190  0.071588           46.628125               41.97                  286.0                  167.0             43.0         44.7       0.0                           7.19       20.02      5.5                   21.0                  5.91                      3.71                              532.0          0.006015                           0.541455  27.0  \n",
       "491            104.0               112.0            14.15              67.56  3.633580     2.99          0.315678       2.85       6.070       18.26            20.0           13.0         3.9   8.4  2.153846     1040.0         425.0                  1.4   0.403458  0.066667           76.835714               29.05                  594.0                  137.0             31.0         21.0       0.0                           6.63      113.10      5.4                   12.0                  5.02                      3.81                              452.0          0.003097                           0.278884  22.0  \n",
       "848             86.0                84.0             5.94              44.82  1.261333     9.52          0.369237       1.44       4.089       14.09            22.0           16.0         2.7   8.1  3.000000      155.0         319.0                  1.0   0.288184  0.013928          492.739997               32.92                  323.0                  293.0            142.0         71.8       0.0                           1.36       12.83      5.2                   32.0                  1.61                      3.67                              522.0          0.001916                           0.621118  26.0  \n",
       "333             86.0                 NaN             4.45                NaN  0.944938      NaN          0.387168       1.17       5.510       22.50             9.0           13.0         8.2   9.5  1.158537      358.0         276.0                  1.3   0.374640  0.018414          226.884614                 NaN                  147.0                    NaN             85.0         70.6       0.0                           1.84        7.70      NaN                    NaN                  2.04                      0.64                              299.0          0.004348                           0.637255   NaN  \n",
       "559             94.0                47.0            12.61              22.28  2.926765     8.97          0.325326       2.50       5.140       16.21            11.0           10.0         5.5   5.9  1.072727      504.0         421.0                  2.2   0.634006  0.060606           29.968182               45.62                  341.0                    NaN             19.0         36.3       0.0                           6.17       24.53      4.9                   12.0                  4.85                      3.51                              540.0          0.004074                           0.453608  18.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file_path = DATASET_PATH\n",
    "df = pd.read_csv(dataset_file_path)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous and Categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_attributes = [\n",
    "#     'PCO 0-healthy control, 1-PCOS, 2-FHA 3-POF, 4-High Andro', 'Hypertension', 'WC>88', 'WHR>0,85 (WHO)', 'WHR>0,8 (NIDDK)', 'WHTR>0,5', \n",
    "#     'overweight/obesity 0-normal/low, 1-overweight, 2-obesity', 'irregular cycles (0-nie, 1-tak)', 'ovulation (0-brak, 1-obecna)', 'PCO ovary morfology in USG (0-brak, 1--obecna)',\n",
    "#     'stromal hypertrophy in ovary (0-brak, 1-obecny)', 'acne', 'hirsutism', 'hyperandrogenism', 'hypothyroidism', 'nodules 0-lack, 1-RL,  2-LL, 3-both', 'chronic thyroiditis',\n",
    "#     'follicules >12', 'hyperlipidemia', 'elevated LDL and TG', 'CHOL>200', 'HDL<50', 'LDL>135', 'TG>150', 'Impaired Fasting Glucose ', 'Impaired Glucose Tolerance', \n",
    "#     'month of birth', 'quarter of the year',\n",
    "#     ]\n",
    "\n",
    "# continuous_attributes = list(df.columns.drop(categorical_attributes))\n",
    "\n",
    "\n",
    "# df[categorical_attributes] = df[categorical_attributes].astype('Int8')\n",
    "# df[continuous_attributes] = df[continuous_attributes].astype('Float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputers pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best preprocessing techniques for vitamine D:\n",
    "- PowerTransformer\n",
    "- IterativeImputer\n",
    "- Advanced one hot encoder\n",
    "- Lasso > DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_attributes_except(attribute):\n",
    "    remaining_attributes = CONTINUOUS_ATTRIBUTES.copy()\n",
    "    remaining_attributes.remove(attribute)\n",
    "    return remaining_attributes\n",
    "\n",
    "\n",
    "class Scaler:\n",
    "    def __init__(self, scaling_method, attributes_to_scale):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.attributes_to_scale = attributes_to_scale\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('scaler', self.scaling_method, self.attributes_to_scale)\n",
    "            ],\n",
    "            # so that the categorical columns remain unchanged\n",
    "            remainder='passthrough',\n",
    "            # so it doesn't rename the columns\n",
    "            verbose_feature_names_out=False,\n",
    "            )\n",
    "        self.scaler.set_output(transform='pandas')\n",
    "        self.scaler.fit(X)\n",
    "\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.scaler = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('scaler', self.scaling_method, self.attributes_to_scale)\n",
    "            ],\n",
    "            # so that the categorical columns remain unchanged\n",
    "            remainder='passthrough',\n",
    "            # so it doesn't rename the columns\n",
    "            verbose_feature_names_out=False,\n",
    "            )\n",
    "        self.scaler.set_output(transform='pandas')\n",
    "        return self.scaler.fit_transform(X)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if not self.scaler:\n",
    "            return RuntimeError('Call fit_tranform() method first')\n",
    "        return self.scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputer:\n",
    "    def __init__(self, imputing_method_continuous, imputing_methohd_categorical, continuous_attributes, categorical_attributes):\n",
    "        self.imputing_method_continuous = imputing_method_continuous\n",
    "        self.imputing_method_categorical = imputing_methohd_categorical\n",
    "        self.continuous_attributes = continuous_attributes\n",
    "        self.categorical_attributes = categorical_attributes\n",
    "        self.imputer_continuous = None\n",
    "        self.imputer_categorical = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer_continuous = self.imputing_method_continuous\n",
    "        self.imputer_continuous.set_output(transform='pandas')\n",
    "        self.imputer_continuous.fit(X[self.continuous_attributes])\n",
    "\n",
    "        self.imputer_categorical = self.imputing_method_categorical\n",
    "        self.imputer_categorical.set_output(transform='pandas')\n",
    "        self.imputer_categorical.fit(X[self.categorical_attributes]).astype('Int8')\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.imputer_continuous = self.imputing_method_continuous\n",
    "        self.imputer_continuous.set_output(transform='pandas')\n",
    "        X_continuous_imputed = self.imputer_continuous.fit_transform(X[self.continuous_attributes])\n",
    "\n",
    "        self.imputer_categorical = self.imputing_method_categorical\n",
    "        self.imputer_categorical.set_output(transform='pandas')\n",
    "        X_categorical_imputed = self.imputer_categorical.fit_transform(X[self.categorical_attributes]).astype('Int8')\n",
    "\n",
    "        return pd.concat([X_continuous_imputed, X_categorical_imputed], axis=1)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if not self.imputer_continuous or not self.imputer_categorical:\n",
    "            return RuntimeError('Call fit_tranform() method first')\n",
    "        X_continuous_imputed = self.imputer_continuous.transform(X[self.continuous_attributes])\n",
    "        X_categorical_imputed = self.imputer_categorical.transform(X[self.categorical_attributes]).astype('Int8')\n",
    "\n",
    "        return pd.concat([X_continuous_imputed, X_categorical_imputed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def run_imputers_comparison_pipeline():\n",
    "    model_scores_df = pd.DataFrame(columns=['imputer', 'train_mae', 'test_mae', \"test_mse\", \"test_rmse\"])\n",
    "    \n",
    "    # TODO: drop some random rows\n",
    "\n",
    "\n",
    "    # scaling\n",
    "    attributes_to_scale = CONTINUOUS_ATTRIBUTES\n",
    "    scaler = Scaler(PowerTransformer(), attributes_to_scale)\n",
    "\n",
    "    # missing data imputation\n",
    "    continuous_imputers = {\n",
    "         'LinearRegression': IterativeImputer(max_iter=30, tol=0.01, initial_strategy='median'),\n",
    "         'RF 10': IterativeImputer(max_iter=30, tol=0.01, initial_strategy='median', estimator=RandomForestRegressor(n_estimators=10, max_depth=10, random_state=42)),\n",
    "         'RF 50': IterativeImputer(max_iter=30, tol=0.01, initial_strategy='median', estimator=RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)),\n",
    "         'RF 100': IterativeImputer(max_iter=30, tol=0.01, initial_strategy='median', estimator=RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)),\n",
    "    }\n",
    "\n",
    "    for continuous_imputer_name, continuous_imputer in continuous_imputers.items():\n",
    "        imputer = Imputer(IterativeImputer(continuous_imputer, KNNImputer(n_neighbors=1), attributes_to_scale, categorical_attributes))\n",
    "\n",
    "        # one hot encoding\n",
    "        one_hot_encoder = OneHotEncoder(columns_for_one_hot_encoding, new_column_names_map, advanced_encoding=True) \n",
    "\n",
    "    # prediction\n",
    "    for model in [DecisionTreeRegressor(max_depth=5, random_state=42), Lasso(), Ridge()]:\n",
    "            \n",
    "            pipeline = make_pipeline(scaler, imputer, one_hot_encoder, model)\n",
    "            # TODO: repeat spowrotem na 3\n",
    "            rkf = RepeatedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "\n",
    "            for train, test in rkf.split(X_train, y_train):\n",
    "                X_train_fold, y_train_fold = X_train.iloc[train], y_train.iloc[train]\n",
    "                X_test_fold, y_test_fold = X_train.iloc[test], y_train.iloc[test]\n",
    "                print('1')\n",
    "\n",
    "                pipeline.fit(X_train_fold, y_train_fold)\n",
    "                y_train_fold_pred = pipeline.predict(X_train_fold)\n",
    "\n",
    "                y_test_fold_pred = pipeline.predict(X_test_fold)\n",
    "                \n",
    "                train_mse = round(mean_squared_error(y_train_fold, y_train_fold_pred), 3)\n",
    "\n",
    "                test_mse = round(mean_squared_error(y_test_fold, y_test_fold_pred), 3)\n",
    "                test_mae = round(mean_absolute_error(y_test_fold, y_test_fold_pred), 3)\n",
    "                test_rmse = round(root_mean_squared_error(y_test_fold, y_test_fold_pred), 3)\n",
    "\n",
    "                # validation\n",
    "                # pipeline.fit(X_train, y_train)\n",
    "                # y_val_pred = pipeline.predict(X_val)\n",
    "                # val_mae = round(mean_absolute_error(y_val. y_val_pred), 3)\n",
    "\n",
    "                model_scores_df.loc[len(model_scores_df)] = [str(model), train_mse, test_mse, test_mae, test_rmse]\n",
    "\n",
    "\n",
    "    return model_scores_df\n",
    "\n",
    "\n",
    "vit_d_model_scores_df = run_model_pipeline(VITAMINE_D)\n",
    "vit_d_model_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mse</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor(max_depth=5, random_state=42)</th>\n",
       "      <td>65.4658</td>\n",
       "      <td>181.5436</td>\n",
       "      <td>10.4520</td>\n",
       "      <td>13.4476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso()</th>\n",
       "      <td>118.2466</td>\n",
       "      <td>128.2492</td>\n",
       "      <td>8.8162</td>\n",
       "      <td>11.3124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    train_mse  test_mse  test_mae  test_rmse\n",
       "model                                                                                       \n",
       "DecisionTreeRegressor(max_depth=5, random_state...    65.4658  181.5436   10.4520    13.4476\n",
       "Lasso()                                              118.2466  128.2492    8.8162    11.3124"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_d_model_scores_df.groupby('model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>60.096</td>\n",
       "      <td>193.035</td>\n",
       "      <td>10.513</td>\n",
       "      <td>13.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>74.899</td>\n",
       "      <td>195.416</td>\n",
       "      <td>11.168</td>\n",
       "      <td>13.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>55.390</td>\n",
       "      <td>208.623</td>\n",
       "      <td>11.706</td>\n",
       "      <td>14.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>72.597</td>\n",
       "      <td>151.446</td>\n",
       "      <td>9.591</td>\n",
       "      <td>12.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>64.347</td>\n",
       "      <td>184.868</td>\n",
       "      <td>10.427</td>\n",
       "      <td>13.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>77.889</td>\n",
       "      <td>193.740</td>\n",
       "      <td>10.734</td>\n",
       "      <td>13.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>60.866</td>\n",
       "      <td>206.816</td>\n",
       "      <td>10.845</td>\n",
       "      <td>14.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>58.824</td>\n",
       "      <td>250.895</td>\n",
       "      <td>11.741</td>\n",
       "      <td>15.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>57.009</td>\n",
       "      <td>209.356</td>\n",
       "      <td>11.096</td>\n",
       "      <td>14.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>77.928</td>\n",
       "      <td>179.642</td>\n",
       "      <td>10.349</td>\n",
       "      <td>13.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>73.630</td>\n",
       "      <td>158.919</td>\n",
       "      <td>10.404</td>\n",
       "      <td>12.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>62.995</td>\n",
       "      <td>251.219</td>\n",
       "      <td>12.228</td>\n",
       "      <td>15.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>79.254</td>\n",
       "      <td>248.600</td>\n",
       "      <td>11.351</td>\n",
       "      <td>15.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>58.974</td>\n",
       "      <td>187.719</td>\n",
       "      <td>10.553</td>\n",
       "      <td>13.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, random_stat...</td>\n",
       "      <td>77.306</td>\n",
       "      <td>185.598</td>\n",
       "      <td>10.500</td>\n",
       "      <td>13.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>117.205</td>\n",
       "      <td>124.294</td>\n",
       "      <td>8.822</td>\n",
       "      <td>11.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>116.676</td>\n",
       "      <td>129.752</td>\n",
       "      <td>9.525</td>\n",
       "      <td>11.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>112.653</td>\n",
       "      <td>148.435</td>\n",
       "      <td>9.373</td>\n",
       "      <td>12.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>124.175</td>\n",
       "      <td>110.913</td>\n",
       "      <td>8.139</td>\n",
       "      <td>10.532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>120.524</td>\n",
       "      <td>127.852</td>\n",
       "      <td>8.222</td>\n",
       "      <td>11.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>117.769</td>\n",
       "      <td>123.658</td>\n",
       "      <td>8.833</td>\n",
       "      <td>11.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>125.144</td>\n",
       "      <td>96.135</td>\n",
       "      <td>7.907</td>\n",
       "      <td>9.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>109.508</td>\n",
       "      <td>165.831</td>\n",
       "      <td>9.591</td>\n",
       "      <td>12.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>118.839</td>\n",
       "      <td>127.043</td>\n",
       "      <td>9.337</td>\n",
       "      <td>11.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>118.125</td>\n",
       "      <td>134.055</td>\n",
       "      <td>8.753</td>\n",
       "      <td>11.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>122.804</td>\n",
       "      <td>98.445</td>\n",
       "      <td>8.047</td>\n",
       "      <td>9.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>107.135</td>\n",
       "      <td>184.887</td>\n",
       "      <td>9.945</td>\n",
       "      <td>13.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>121.343</td>\n",
       "      <td>121.214</td>\n",
       "      <td>8.634</td>\n",
       "      <td>11.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>118.859</td>\n",
       "      <td>120.265</td>\n",
       "      <td>8.859</td>\n",
       "      <td>10.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>120.303</td>\n",
       "      <td>114.234</td>\n",
       "      <td>8.517</td>\n",
       "      <td>10.688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  train_mse  val_mse  val_mae  val_rmse\n",
       "0   DecisionTreeRegressor(max_depth=5, random_stat...     60.096  193.035   10.513    13.894\n",
       "1   DecisionTreeRegressor(max_depth=5, random_stat...     74.899  195.416   11.168    13.979\n",
       "2   DecisionTreeRegressor(max_depth=5, random_stat...     55.390  208.623   11.706    14.444\n",
       "3   DecisionTreeRegressor(max_depth=5, random_stat...     72.597  151.446    9.591    12.306\n",
       "4   DecisionTreeRegressor(max_depth=5, random_stat...     64.347  184.868   10.427    13.597\n",
       "5   DecisionTreeRegressor(max_depth=5, random_stat...     77.889  193.740   10.734    13.919\n",
       "6   DecisionTreeRegressor(max_depth=5, random_stat...     60.866  206.816   10.845    14.381\n",
       "7   DecisionTreeRegressor(max_depth=5, random_stat...     58.824  250.895   11.741    15.840\n",
       "8   DecisionTreeRegressor(max_depth=5, random_stat...     57.009  209.356   11.096    14.469\n",
       "9   DecisionTreeRegressor(max_depth=5, random_stat...     77.928  179.642   10.349    13.403\n",
       "10  DecisionTreeRegressor(max_depth=5, random_stat...     73.630  158.919   10.404    12.606\n",
       "11  DecisionTreeRegressor(max_depth=5, random_stat...     62.995  251.219   12.228    15.850\n",
       "12  DecisionTreeRegressor(max_depth=5, random_stat...     79.254  248.600   11.351    15.767\n",
       "13  DecisionTreeRegressor(max_depth=5, random_stat...     58.974  187.719   10.553    13.701\n",
       "14  DecisionTreeRegressor(max_depth=5, random_stat...     77.306  185.598   10.500    13.623\n",
       "15                                            Lasso()    117.205  124.294    8.822    11.149\n",
       "16                                            Lasso()    116.676  129.752    9.525    11.391\n",
       "17                                            Lasso()    112.653  148.435    9.373    12.183\n",
       "18                                            Lasso()    124.175  110.913    8.139    10.532\n",
       "19                                            Lasso()    120.524  127.852    8.222    11.307\n",
       "20                                            Lasso()    117.769  123.658    8.833    11.120\n",
       "21                                            Lasso()    125.144   96.135    7.907     9.805\n",
       "22                                            Lasso()    109.508  165.831    9.591    12.878\n",
       "23                                            Lasso()    118.839  127.043    9.337    11.271\n",
       "24                                            Lasso()    118.125  134.055    8.753    11.578\n",
       "25                                            Lasso()    122.804   98.445    8.047     9.922\n",
       "26                                            Lasso()    107.135  184.887    9.945    13.597\n",
       "27                                            Lasso()    121.343  121.214    8.634    11.010\n",
       "28                                            Lasso()    118.859  120.265    8.859    10.967\n",
       "29                                            Lasso()    120.303  114.234    8.517    10.688"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_d_model_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mse</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_rmse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor(max_depth=5, random_state=42)</th>\n",
       "      <td>67.466933</td>\n",
       "      <td>200.392800</td>\n",
       "      <td>10.8804</td>\n",
       "      <td>14.1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso()</th>\n",
       "      <td>118.070800</td>\n",
       "      <td>128.467533</td>\n",
       "      <td>8.8336</td>\n",
       "      <td>11.2932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     train_mse     val_mse  val_mae  val_rmse\n",
       "model                                                                                        \n",
       "DecisionTreeRegressor(max_depth=5, random_state...   67.466933  200.392800  10.8804   14.1186\n",
       "Lasso()                                             118.070800  128.467533   8.8336   11.2932"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  with a default iterativeimputer linearRegression estimator\n",
    "vit_d_model_scores_df.groupby('model').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla każdego z modeli przetestować różne parametry i stworzyć wykresy dla każdego parametru:\n",
    "- np alpha parameter (x) vs mae (y)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.118e+00, tolerance: 4.989e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([109.16592369,  62.32950935,  62.33647876,  66.22161074]),\n",
       " 'std_fit_time': array([76.382824  ,  0.39626516,  0.23494729,  6.14108938]),\n",
       " 'mean_score_time': array([0.3219862 , 0.19005113, 0.16095967, 0.20741949]),\n",
       " 'std_score_time': array([0.21601786, 0.04149761, 0.01517507, 0.06198451]),\n",
       " 'param_model__alpha': masked_array(data=[0.1, 1.0, 10.0, 100.0],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__alpha': 0.1},\n",
       "  {'model__alpha': 1.0},\n",
       "  {'model__alpha': 10.0},\n",
       "  {'model__alpha': 100.0}],\n",
       " 'split0_test_score': array([-8.84589879, -8.66739971, -9.51911856, -9.51911856]),\n",
       " 'split1_test_score': array([-10.01262553,  -9.01997728,  -9.64363749,  -9.64363749]),\n",
       " 'split2_test_score': array([-9.92383366, -8.69328409, -8.97653631, -8.97653631]),\n",
       " 'split3_test_score': array([-9.17515054, -8.57576041, -8.83496604, -8.83496604]),\n",
       " 'split4_test_score': array([-9.40441489, -8.93567443, -9.28146224, -9.28146224]),\n",
       " 'mean_test_score': array([-9.47238468, -8.77841918, -9.25114413, -9.25114413]),\n",
       " 'std_test_score': array([0.44297155, 0.16954257, 0.30834856, 0.30834856]),\n",
       " 'rank_test_score': array([4, 1, 2, 2])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hyperparameter_tuning(target_attribute, model, param_grid):\n",
    "    # model_scores_df = pd.DataFrame(columns=['model', 'alpha', 'train_mae', \"test_mae\", 'test_mse', \"test_rmse\"])\n",
    "    # drop rows with missing target_attribute\n",
    "    cleaned_df = df.dropna(subset=[target_attribute])\n",
    "    \n",
    "    X, y = cleaned_df.drop(columns=[target_attribute]), cleaned_df[target_attribute]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # scaling\n",
    "    attributes_to_scale = get_continuous_attributes_except(target_attribute)\n",
    "    scaler = Scaler(PowerTransformer(), attributes_to_scale)\n",
    "\n",
    "    # missing data imputation\n",
    "    imputer = Imputer(IterativeImputer(max_iter=40, tol=0.01, initial_strategy='median'),\n",
    "                                        # TODO: experiment with different estimators and increase max_iter\n",
    "                                    #    estimator=RandomForestRegressor(n_estimators=10,\n",
    "                                    #                                     max_depth=10,\n",
    "                                    #                                     bootstrap=True,\n",
    "                                    #                                     max_samples=0.5,\n",
    "                                    #                                     n_jobs=2,\n",
    "                                    #                                     random_state=42)),\n",
    "                    KNNImputer(n_neighbors=1),\n",
    "                    attributes_to_scale,\n",
    "                    categorical_attributes)\n",
    "\n",
    "    # one hot encoding\n",
    "    one_hot_encoder = OneHotEncoder(columns_for_one_hot_encoding, new_column_names_map, advanced_encoding=True) # TODO: try with advanced encoding \n",
    "\n",
    "    pipeline = Pipeline([('scaler', scaler), ('imputer', imputer), ('one_hot_encoder', one_hot_encoder), ('model', model)])\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error', return_train_score=True).fit(X_train, y_train)\n",
    "\n",
    "    return grid_search.cv_results_\n",
    "\n",
    "\n",
    "model = Lasso()\n",
    "\n",
    "param_grid = {\n",
    "    \"model__alpha\": 10.0 ** np.arange(-1, 3),\n",
    "    #  \"model__alpha\": 10.0 ** np.arange(-5, 6),\n",
    "              }\n",
    "\n",
    "vit_d_model_hyperparam_tuning_df = hyperparameter_tuning(VITAMINE_D, model, param_grid)\n",
    "vit_d_model_hyperparam_tuning_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24acf6d75f0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alphas = grid_search['param_model__alpha']\n",
    "\n",
    "# mean_test_score = grid_search['mean_test_score']\n",
    "# mean_train_score = grid_search['mean_train_score']\n",
    "\n",
    "\n",
    "alphas = [0.1, 1, 10, 100]\n",
    "mean_test_score = [-9.47238468, -8.77841918, -9.25114413, -9.25114413]\n",
    "mean_train_score = [-7.47238468, -6.77841918, -8.25114413, -7.25114413]\n",
    "\n",
    "results_df = pd.DataFrame({'alpha': alphas, 'mean_test_score': mean_test_score, 'mean_train_score': mean_train_score})\n",
    "\n",
    "\n",
    "plt.plot(alphas, mean_test_score)\n",
    "plt.plot(alphas, mean_train_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='alpha', ylabel='value'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x='alpha', y='value', hue='variable', \n",
    "             data=pd.melt(results_df, ['alpha']),\n",
    "             palette=['red', 'blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fajny wykres przedstawiający liczbę zmiennych - może zbyt duży na naszą liczbę zmiennych\n",
    "\n",
    "coefs=[]\n",
    "alphas=2**np.arange(5,-10,-.1)\n",
    "for alpha in alphas:\n",
    "    ridge= Ridge(alpha=alpha, max_iter=100000).fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "coefs=np.array(coefs)\n",
    "\n",
    "#plot results\n",
    "N,M=X_train.shape\n",
    "plt.figure(figsize=(10,7))\n",
    "for i in range(M):\n",
    "    plt.plot(alphas, coefs[:,i], label=\"Var %d\" % (i+1))\n",
    "plt.axvline(x = 0.1, color = 'b', label = 'best Alpha')\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection for IGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection for prolactin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_preprocessed, y_train)\n",
    "importances = np.abs(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "selector = SelectFromModel(model, threshold=0.1)\n",
    "selector.fit(X_train_preprocessed, y_train)\n",
    "selected_features = X_train_preprocessed.columns[selector.get_support()]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- birth_month_-0.2\n- birth_month_-0.4\n- birth_month_-0.6\n- birth_month_-0.8\n- birth_month_-1.0\n- ...\nFeature names seen at fit time, yet now missing:\n- birth_month_-0.16666666666666666\n- birth_month_-0.3333333333333333\n- birth_month_-0.5\n- birth_month_-0.6666666666666666\n- birth_month_-0.8333333333333334\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m importances \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mimportances_mean\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # Associate each importance value with its corresponding feature name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\inspection\\_permutation_importance.py:290\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[1;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[0;32m    287\u001b[0m     scorers_dict \u001b[38;5;241m=\u001b[39m _check_multimetric_scoring(estimator, scoring)\n\u001b[0;32m    288\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m _MultimetricScorer(scorers\u001b[38;5;241m=\u001b[39mscorers_dict)\n\u001b[1;32m--> 290\u001b[0m baseline_score \u001b[38;5;241m=\u001b[39m \u001b[43m_weights_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m scores \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)(\n\u001b[0;32m    293\u001b[0m     delayed(_calculate_permutation_scores)(\n\u001b[0;32m    294\u001b[0m         estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    305\u001b[0m )\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(baseline_score, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\inspection\\_permutation_importance.py:26\u001b[0m, in \u001b[0;36m_weights_scorer\u001b[1;34m(scorer, estimator, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scorer(estimator, X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:415\u001b[0m, in \u001b[0;36m_PassthroughScorer.__call__\u001b[1;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    414\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\base.py:848\u001b[0m, in \u001b[0;36mRegressorMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the coefficient of determination of the prediction.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \n\u001b[0;32m    808\u001b[0m \u001b[38;5;124;03mThe coefficient of determination :math:`R^2` is defined as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03m:class:`~sklearn.multioutput.MultiOutputRegressor`).\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[1;32m--> 848\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r2_score(y, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:529\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    528\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 529\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    531\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:489\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    497\u001b[0m     X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[0;32m    498\u001b[0m ):\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\karin\\.Studia\\magisterka\\Master_project\\.venv\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- birth_month_-0.2\n- birth_month_-0.4\n- birth_month_-0.6\n- birth_month_-0.8\n- birth_month_-1.0\n- ...\nFeature names seen at fit time, yet now missing:\n- birth_month_-0.16666666666666666\n- birth_month_-0.3333333333333333\n- birth_month_-0.5\n- birth_month_-0.6666666666666666\n- birth_month_-0.8333333333333334\n- ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = permutation_importance(model, X_test_preprocessed, y_test, n_repeats=10, random_state=42)\n",
    "importances = result.importances_mean\n",
    "\n",
    "# # Associate each importance value with its corresponding feature name\n",
    "feature_importances = dict(zip(X_test_preprocessed.columns, importances))\n",
    "\n",
    "# Print the columns together with their importances\n",
    "for feature, importance in feature_importances.items():\n",
    "    if importance != 0:\n",
    "        print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHR (Waist/Hip ratio): 1\n",
      "% lymphocytes : 1\n",
      "P-LCR %: 1\n",
      "HOMA: 1\n",
      "age: 1\n",
      "ALT U/l: 2\n",
      "hematocrit [%]: 3\n",
      "PLT x10^3/ul: 4\n",
      "Estradiol pg/ml: 5\n",
      "LDL mmol/l: 6\n",
      "MCH pg: 7\n",
      "glucose  120 mg/dl: 8\n",
      "Dihydrotestosterone pg/ml (N<368): 9\n",
      "MPV fl: 10\n",
      "CRP mg/l: 11\n",
      "LH/FSH: 12\n",
      "FT4 pmol/l: 13\n",
      "FT3 pmol/l: 14\n",
      "VAI - Visceral adiposity index: 15\n",
      "ovaries volume - total: 16\n",
      "neutrophil x10^3/ul: 17\n",
      "RDW-CV %: 18\n",
      "Matsuda: 19\n",
      "TIBC: 20\n",
      "Anty-TG IU/ml: 21\n",
      "cortisol nmol/l  8:00: 22\n",
      "TSH mIU/L: 23\n",
      "T/SHBG: 24\n",
      "thyroid volume: 25\n",
      "CHOL mg/dl : 26\n",
      "overweight_1.0: 27\n",
      "FG score (Ferriman-Gallway score - stopień androgenizacji): 28\n",
      "Hemoglobin [g/dl]: 29\n",
      "lymphocytes x10^3/ul: 30\n",
      "Anty-TPO IU/ml: 31\n",
      "AST U/l: 32\n",
      "Vole of the Right Ovary: 33\n",
      "DHEA-S ug/dl: 34\n",
      "nodules_0.0: 35\n",
      "T (ng/ml): 36\n",
      "potassium mmol/l: 37\n",
      "SHBG nmol/l: 38\n",
      "Atherogenic index (AI) (LDL-C/HDL-C) : 39\n",
      "Volume of the thyroid  Left Lobe: 40\n",
      "Parathormone pg/ml: 41\n",
      "BAI - Body adiposity index: 42\n",
      "% neutrophil : 43\n",
      "QUICKI (N<0,357): 44\n",
      "glucose 0 mg/dl: 45\n",
      "birth_month_0.6: 46\n",
      "Testosterone/DHT: 47\n",
      "limf/mono: 48\n",
      "MCV fl: 49\n",
      "HTC/Hb: 50\n",
      "height (cm): 51\n",
      "FSH mlU/ml: 52\n",
      "systolic BP (ciśnienie skurczowe): 53\n",
      "PLR: 54\n",
      "creatinine mg/dl: 55\n",
      "stromal hypertrophy in ovary (0-brak, 1-obecny): 56\n",
      "17-OH-progesterone ng/ml: 57\n",
      "eos/leukocyty: 58\n",
      "monocytes x10^3/ul: 59\n",
      "TG: 60\n",
      "WHTR (Waist/Height Ratio): 61\n",
      "RBC x10^6ul: 62\n",
      "MPV/PLT: 63\n",
      "prolactin: 64\n",
      "Hip Circumference (HC): 65\n",
      "sodium mmol/l: 66\n",
      "Androstendione ng/ml: 67\n",
      "Bilirubin mg/dl: 68\n",
      "FTI (free testosterone index): 69\n",
      "ACTH pg/ml: 70\n",
      "eosinocytes x10^3/ul: 71\n",
      "coronary risk index (CRI) (TG/HDL-C): 72\n",
      "Waist Circumference (WC): 73\n",
      "LH: 74\n",
      "PLT/WBC: 75\n",
      "TSAT: 76\n",
      "MCHC g/dl: 77\n",
      "% monocytes: 78\n",
      "T/A (testosterone/androstendione): 79\n",
      "NLR (stosunek neutrofili do limfocytów): 80\n",
      "cortisol nmol/l 18:00: 81\n",
      "TyG Index - Trigliceride-glucose index: 82\n",
      "E(pg/ml)/T(ng/ml)/: 83\n",
      "HbA1c %: 84\n",
      "UIBC ug/dl: 85\n",
      "LAP INDEX - Lipid accumulation product index: 86\n",
      "Volume of the thyroid  Right Lobe: 87\n",
      "diastolic BP (ciśnienie rozskurczowe): 88\n",
      "LDL mg/dl: 89\n",
      "BMI: 90\n",
      "calcium mg/dl: 91\n",
      "WBC x10^3/ul: 92\n",
      "%eosinocytes : 93\n",
      "PDW fl: 94\n",
      "vitamin 25-OH D ng/ml: 95\n",
      "L/WCC (leukocyty do całkowitej liczby krwinek białych): 96\n",
      "ferritin ng/ml: 97\n",
      "birth_quarter_0.0: 98\n",
      "CHOL mmol/l: 99\n",
      "ferrum ug/dl: 100\n",
      "WHR>0,85 (WHO): 101\n",
      "Insulin 120 uU/ml: 102\n",
      "acne: 103\n",
      "phosphorus mg/dl: 104\n",
      "testosterone nmol/l: 105\n",
      "birth_quarter_-1.0: 106\n",
      "Hypertension: 107\n",
      "AIP -Atherogenic index of plasma: 108\n",
      "WC>88: 109\n",
      "WHTR>0,5: 110\n",
      "TG mmol/l: 111\n",
      "follicules >12: 112\n",
      "weight: 113\n",
      "Impaired Glucose Tolerance: 114\n",
      "hypothyroidism: 115\n",
      "birth_month_0.0: 116\n",
      "overweight_2.0: 117\n",
      "chronic thyroiditis: 118\n",
      "insulin 0 uU/ml: 119\n",
      "WHR>0,8 (NIDDK): 120\n",
      "%basophils : 121\n",
      "birth_month_-0.6: 122\n",
      "birth_month_-0.4: 123\n",
      "birth_quarter_2.0: 124\n",
      "birth_month_0.2: 125\n",
      "birth_month_-1.0: 126\n",
      "birth_month_-0.2: 127\n",
      "basophils x10^3/ul: 128\n",
      "HDL mmol/l: 129\n",
      " HDL mg/dl: 130\n",
      "birth_month_-0.8: 131\n",
      "ovulation (0-brak, 1-obecna): 132\n",
      "PCO ovary morfology in USG (0-brak, 1--obecna): 133\n",
      "AMH (ng/ml) *7,14=pmol/l: 134\n",
      "CHOL>200: 135\n",
      "overweight_0.0: 136\n",
      "PCO_1.0: 137\n",
      "TG>150: 138\n",
      "nodules_3.0: 139\n",
      "PCO_2.0: 140\n",
      "Volume of the  Left Ovary: 141\n",
      "birth_month_0.4: 142\n",
      "NRBC x10^3/ul: 143\n",
      "birth_quarter_1.0: 144\n",
      "elevated LDL and TG: 145\n",
      "hyperandrogenism: 146\n",
      "irregular cycles (0-nie, 1-tak): 147\n",
      "PCO_4.0: 148\n",
      "LDL>135: 149\n",
      "nodules_2.0: 150\n",
      "hirsutism: 151\n",
      "proBNP: 152\n",
      "PCO_3.0: 153\n",
      "PCO_0.0: 154\n",
      "HDL<50: 155\n",
      "nodules_1.0: 156\n",
      "Impaired Fasting Glucose : 157\n",
      "birth_month_0.8: 158\n",
      "hyperlipidemia: 159\n",
      "birth_month_1.0: 160\n",
      "birth_month_1.2: 161\n"
     ]
    }
   ],
   "source": [
    "rfe = RFE(estimator=model, n_features_to_select=5)\n",
    "rfe.fit(X_train_preprocessed, y_train)\n",
    "importances = rfe.ranking_\n",
    "\n",
    "feature_importances = dict(zip(X_test_preprocessed.columns, importances))\n",
    "\n",
    "sorted_feature_importances = sorted(feature_importances.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the columns together with their importances\n",
    "for feature, importance in sorted_feature_importances:\n",
    "    if importance != 0:\n",
    "        print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. Feature 'age' (0.11558109256072505)\n",
      "2. Feature '% lymphocytes ' (0.037067424059781806)\n",
      "3. Feature '17-OH-progesterone ng/ml' (0.02265471763561137)\n",
      "4. Feature 'insulin 0 uU/ml' (0.02005921061817757)\n",
      "5. Feature 'ALT U/l' (0.020031333707779757)\n",
      "6. Feature 'TIBC' (0.018341271049474084)\n",
      "7. Feature 'P-LCR %' (0.0180263764801652)\n",
      "8. Feature 'DHEA-S ug/dl' (0.017381523781329275)\n",
      "9. Feature 'E(pg/ml)/T(ng/ml)/' (0.013925019525298177)\n",
      "10. Feature 'Dihydrotestosterone pg/ml (N<368)' (0.01381382592287395)\n",
      "11. Feature 'FT3 pmol/l' (0.013211499351731826)\n",
      "12. Feature 'MPV fl' (0.013196172022832464)\n",
      "13. Feature 'QUICKI (N<0,357)' (0.012496564375672798)\n",
      "14. Feature 'ferritin ng/ml' (0.011887067391901414)\n",
      "15. Feature 'Hemoglobin [g/dl]' (0.011773180214576847)\n",
      "16. Feature 'WHTR (Waist/Height Ratio)' (0.010941850797454569)\n",
      "17. Feature 'neutrophil x10^3/ul' (0.01090754824336664)\n",
      "18. Feature 'phosphorus mg/dl' (0.010880616821887323)\n",
      "19. Feature 'SHBG nmol/l' (0.010875881110953065)\n",
      "20. Feature 'proBNP' (0.01071635193803706)\n",
      "21. Feature 'RBC x10^6ul' (0.010592815329138234)\n",
      "22. Feature 'PDW fl' (0.010527996443196087)\n",
      "23. Feature 'systolic BP (ciśnienie skurczowe)' (0.010407919628337592)\n",
      "24. Feature 'WHR (Waist/Hip ratio)' (0.010217992129736893)\n",
      "25. Feature 'Parathormone pg/ml' (0.010182204071036106)\n",
      "26. Feature 'MCH pg' (0.009916983927956058)\n",
      "27. Feature 'Volume of the  Left Ovary' (0.009878131714021606)\n",
      "28. Feature 'HOMA' (0.009825289992908182)\n",
      "29. Feature 'Vole of the Right Ovary' (0.00976735909854213)\n",
      "30. Feature 'diastolic BP (ciśnienie rozskurczowe)' (0.009570740896758897)\n",
      "31. Feature 'prolactin' (0.009538157732822374)\n",
      "32. Feature 'TSH mIU/L' (0.009390477014504267)\n",
      "33. Feature 'Estradiol pg/ml' (0.009041365370775198)\n",
      "34. Feature 'Matsuda' (0.008957951041376382)\n",
      "35. Feature 'Atherogenic index (AI) (LDL-C/HDL-C) ' (0.008935363626880608)\n",
      "36. Feature 'cortisol nmol/l  8:00' (0.008780842414309637)\n",
      "37. Feature 'FSH mlU/ml' (0.008776006333862824)\n",
      "38. Feature 'FG score (Ferriman-Gallway score - stopień androgenizacji)' (0.00865283452176957)\n",
      "39. Feature 'limf/mono' (0.008612823694155466)\n",
      "40. Feature 'PLR' (0.008438776746981602)\n",
      "41. Feature 'calcium mg/dl' (0.008144183751171652)\n",
      "42. Feature 'WBC x10^3/ul' (0.007926018170301027)\n",
      "43. Feature 'hematocrit [%]' (0.007683340223092829)\n",
      "44. Feature 'ACTH pg/ml' (0.007605392951985356)\n",
      "45. Feature 'vitamin 25-OH D ng/ml' (0.0075265121961946655)\n",
      "46. Feature 'CRP mg/l' (0.007467759085798415)\n",
      "47. Feature 'monocytes x10^3/ul' (0.0074518736190982565)\n",
      "48. Feature 'FTI (free testosterone index)' (0.007081964291823076)\n",
      "49. Feature 'HTC/Hb' (0.007035510339223657)\n",
      "50. Feature 'creatinine mg/dl' (0.00686512789667479)\n",
      "51. Feature 'LAP INDEX - Lipid accumulation product index' (0.006853374349494836)\n",
      "52. Feature 'FT4 pmol/l' (0.006848995696654582)\n",
      "53. Feature 'Testosterone/DHT' (0.006605314524869186)\n",
      "54. Feature 'Bilirubin mg/dl' (0.00658232668695903)\n",
      "55. Feature 'lymphocytes x10^3/ul' (0.006484930845302011)\n",
      "56. Feature 'Androstendione ng/ml' (0.006473469580272111)\n",
      "57. Feature 'MPV/PLT' (0.00631682308417903)\n",
      "58. Feature 'potassium mmol/l' (0.006227924619063531)\n",
      "59. Feature 'BAI - Body adiposity index' (0.006205260444595249)\n",
      "60. Feature 'RDW-CV %' (0.006180537294021612)\n",
      "61. Feature 'height (cm)' (0.006078171174723568)\n",
      "62. Feature 'weight' (0.006053018870714896)\n",
      "63. Feature 'eos/leukocyty' (0.006000527744083287)\n",
      "64. Feature 'BMI' (0.005982940007339762)\n",
      "65. Feature 'MCV fl' (0.00589513866580014)\n",
      "66. Feature '% monocytes' (0.005884648380848445)\n",
      "67. Feature 'LH' (0.005849164725482444)\n",
      "68. Feature 'PLT/WBC' (0.005730644926249408)\n",
      "69. Feature 'eosinocytes x10^3/ul' (0.005704629107024033)\n",
      "70. Feature 'cortisol nmol/l 18:00' (0.005653812468576258)\n",
      "71. Feature 'AST U/l' (0.005587951067684891)\n",
      "72. Feature 'ovaries volume - total' (0.0055745322406047125)\n",
      "73. Feature 'Volume of the thyroid  Left Lobe' (0.005564757414142438)\n",
      "74. Feature 'T/A (testosterone/androstendione)' (0.005537300571965995)\n",
      "75. Feature 'Insulin 120 uU/ml' (0.005520583022612028)\n",
      "76. Feature 'Anty-TPO IU/ml' (0.00531018344099663)\n",
      "77. Feature 'CHOL mmol/l' (0.0052991949614167875)\n",
      "78. Feature 'glucose 0 mg/dl' (0.005295630213716349)\n",
      "79. Feature 'Waist Circumference (WC)' (0.00519699157344015)\n",
      "80. Feature 'TyG Index - Trigliceride-glucose index' (0.0051955056583677635)\n",
      "81. Feature 'Volume of the thyroid  Right Lobe' (0.005028004465652253)\n",
      "82. Feature 'CHOL mg/dl ' (0.005018500621286733)\n",
      "83. Feature '%eosinocytes ' (0.00496368932588299)\n",
      "84. Feature 'glucose  120 mg/dl' (0.004913839186435304)\n",
      "85. Feature 'AMH (ng/ml) *7,14=pmol/l' (0.004778588398341531)\n",
      "86. Feature 'LH/FSH' (0.004753812053434605)\n",
      "87. Feature 'T/SHBG' (0.004515214174927064)\n",
      "88. Feature '%basophils ' (0.0044059059812508435)\n",
      "89. Feature 'HbA1c %' (0.004294787139646982)\n",
      "90. Feature 'LDL mmol/l' (0.004249323287863363)\n",
      "91. Feature 'thyroid volume' (0.0040191654111911895)\n",
      "92. Feature 'MCHC g/dl' (0.0040144169840745595)\n",
      "93. Feature 'sodium mmol/l' (0.0038587580914473643)\n",
      "94. Feature 'TG' (0.003820369334930864)\n",
      "95. Feature 'PLT x10^3/ul' (0.003707143737299619)\n",
      "96. Feature 'Anty-TG IU/ml' (0.0033479690682396155)\n",
      "97. Feature 'LDL mg/dl' (0.00331737875593083)\n",
      "98. Feature 'VAI - Visceral adiposity index' (0.00326279747176754)\n",
      "99. Feature 'UIBC ug/dl' (0.003242371857328288)\n",
      "100. Feature '% neutrophil ' (0.003242170368543133)\n",
      "101. Feature 'L/WCC (leukocyty do całkowitej liczby krwinek białych)' (0.003071867171755026)\n",
      "102. Feature 'testosterone nmol/l' (0.002972662731529576)\n",
      "103. Feature 'TSAT' (0.0028670152565136637)\n",
      "104. Feature 'Hip Circumference (HC)' (0.0028541543965509607)\n",
      "105. Feature ' HDL mg/dl' (0.0028158353580945506)\n",
      "106. Feature 'HDL mmol/l' (0.0027130788529922353)\n",
      "107. Feature 'NLR (stosunek neutrofili do limfocytów)' (0.002522382241837988)\n",
      "108. Feature 'basophils x10^3/ul' (0.0024420382101804053)\n",
      "109. Feature 'ferrum ug/dl' (0.0024224503941904937)\n",
      "110. Feature 'ovulation (0-brak, 1-obecna)' (0.002346669035608583)\n",
      "111. Feature 'coronary risk index (CRI) (TG/HDL-C)' (0.002248054964364613)\n",
      "112. Feature 'nodules_0.0' (0.00219239249147238)\n",
      "113. Feature 'AIP -Atherogenic index of plasma' (0.0021460396676789197)\n",
      "114. Feature 'T (ng/ml)' (0.0021176500069019174)\n",
      "115. Feature 'nodules_3.0' (0.0019885034544232408)\n",
      "116. Feature 'TG mmol/l' (0.0018665915648074134)\n",
      "117. Feature 'stromal hypertrophy in ovary (0-brak, 1-obecny)' (0.0016399774896285054)\n",
      "118. Feature 'overweight_1.0' (0.0015443708446324894)\n",
      "119. Feature 'birth_quarter_1.0' (0.0014503176488823207)\n",
      "120. Feature 'birth_month_-0.16666666666666666' (0.0011070287407385336)\n",
      "121. Feature 'acne' (0.0010517673935728118)\n",
      "122. Feature 'nodules_1.0' (0.0009957962089620618)\n",
      "123. Feature 'PCO ovary morfology in USG (0-brak, 1--obecna)' (0.0008048435583336298)\n",
      "124. Feature 'follicules >12' (0.0008042797630838283)\n",
      "125. Feature 'irregular cycles (0-nie, 1-tak)' (0.0008023099056305508)\n",
      "126. Feature 'chronic thyroiditis' (0.0007488421338787332)\n",
      "127. Feature 'hypothyroidism' (0.0006477448741647111)\n",
      "128. Feature 'birth_month_0.16666666666666666' (0.0006427142211340959)\n",
      "129. Feature 'overweight_2.0' (0.0006040395212120063)\n",
      "130. Feature 'birth_quarter_0.0' (0.0005710293623762166)\n",
      "131. Feature 'overweight_0.0' (0.0005607933781517632)\n",
      "132. Feature 'PCO_1.0' (0.0005236490853217455)\n",
      "133. Feature 'PCO_0.0' (0.0004916813747719442)\n",
      "134. Feature 'PCO_2.0' (0.0004537322415115342)\n",
      "135. Feature 'birth_quarter_-0.5' (0.000426651519986775)\n",
      "136. Feature 'birth_month_0.5' (0.0004043237079138274)\n",
      "137. Feature 'birth_month_0.0' (0.00040430002001700673)\n",
      "138. Feature 'WC>88' (0.000377126011963723)\n",
      "139. Feature 'birth_month_-0.8333333333333334' (0.0003524821839437917)\n",
      "140. Feature 'birth_quarter_0.5' (0.0003113011197324264)\n",
      "141. Feature 'NRBC x10^3/ul' (0.0003085332492839153)\n",
      "142. Feature 'birth_month_0.8333333333333334' (0.00027800194667165713)\n",
      "143. Feature 'birth_month_0.6666666666666666' (0.0002746719872768436)\n",
      "144. Feature 'birth_month_1.0' (0.0002524864158457814)\n",
      "145. Feature 'hyperandrogenism' (0.00023903460812671175)\n",
      "146. Feature 'CHOL>200' (0.00022154280658310234)\n",
      "147. Feature 'WHR>0,85 (WHO)' (0.00021501206237538954)\n",
      "148. Feature 'hirsutism' (0.00021481376038361374)\n",
      "149. Feature 'birth_month_0.3333333333333333' (0.0002064740688659777)\n",
      "150. Feature 'nodules_2.0' (0.00019928451563567127)\n",
      "151. Feature 'HDL<50' (0.00017290077155082578)\n",
      "152. Feature 'Hypertension' (0.00014368759970183136)\n",
      "153. Feature 'WHTR>0,5' (0.0001262570487932468)\n",
      "154. Feature 'PCO_3.0' (9.267131427024933e-05)\n",
      "155. Feature 'PCO_4.0' (8.490679755236804e-05)\n",
      "156. Feature 'hyperlipidemia' (8.397225635853199e-05)\n",
      "157. Feature 'TG>150' (8.247646848310611e-05)\n",
      "158. Feature 'LDL>135' (8.240115963638737e-05)\n",
      "159. Feature 'Impaired Fasting Glucose ' (8.040453856482952e-05)\n",
      "160. Feature 'Impaired Glucose Tolerance' (7.72451463190311e-05)\n",
      "161. Feature 'birth_month_-0.5' (7.582874037611937e-05)\n",
      "162. Feature 'WHR>0,8 (NIDDK)' (5.465126280838118e-05)\n",
      "163. Feature 'elevated LDL and TG' (4.5807083044082e-05)\n",
      "164. Feature 'birth_month_-0.3333333333333333' (4.037345147946341e-05)\n",
      "165. Feature 'birth_month_-0.6666666666666666' (2.1341890826475687e-05)\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train_preprocessed.shape[1]):\n",
    "\n",
    "    print(f\"{f + 1}. Feature '{X_train_preprocessed.columns[indices[f]]}' ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingDataImputer:\n",
    "    def __init__(self, \n",
    "                columns_to_drop_rows_if_missing_value=[],\n",
    "                columns_to_impute_with_median=[],\n",
    "                columns_to_impute_with_knn=[],\n",
    "                columns_to_drop=[]):\n",
    "        self.columns_to_drop_rows_if_missing_value = columns_to_drop_rows_if_missing_value\n",
    "        self.columns_to_impute_with_median = columns_to_impute_with_median\n",
    "        self.columns_to_impute_with_knn = columns_to_impute_with_knn\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "\n",
    "    def impute(self, X, y):\n",
    "        X, y = X.copy(), y.copy()\n",
    "\n",
    "        # Drop rows for which the values in specified columns are missing\n",
    "        if self.columns_to_drop_rows_if_missing_value:\n",
    "            X.reset_index(drop=True, inplace=True)\n",
    "            y.reset_index(drop=True, inplace=True)\n",
    "            X = X.dropna(subset=self.columns_to_drop_rows_if_missing_value)\n",
    "            y = y.loc[X.index] # leave only the relevant rows in y\n",
    "\n",
    "        # Impute specified columns with median\n",
    "        if self.columns_to_impute_with_median:\n",
    "            #  TODO: fit transform only on the training set and only transform on the test set\n",
    "            median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "            X[self.columns_to_impute_with_median] = median_imputer.fit_transform(X[self.columns_to_impute_with_median])\n",
    "\n",
    "        # Impute specified columns with knn\n",
    "        if self.columns_to_impute_with_knn:\n",
    "            #  TODO: fit transform only on the training set and only transform on the test set\n",
    "            knn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "            X[self.columns_to_impute_with_knn] = knn_imputer.fit_transform(X[self.columns_to_impute_with_knn])\n",
    "\n",
    "        # Drop specified columns\n",
    "        if self.columns_to_drop:\n",
    "            X = X.drop(columns=self.columns_to_drop)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
